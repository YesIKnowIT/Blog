= Keep you Secret Secret
:author: Sylvain Leroux
:pin: -
:revnumber: v0.1
:revdate: 2022-06-07T15:55:01+02:00
:keywords: 

[.teaser]
When you work collaboratively on a web application or when you deploy it, there are configuration settings and sensitive pieces of data you don't want to share.
I don't pretend to be exhaustive, but I searched the web for various ways to deal with that, and this article is a compilation of what seems to be the most popular strategies.

== Why is it a problem?

TBD

== My starting point
Would you push your credit card number into a Git repository? Certainly no.
Even if that repository is private, and you trust all the members of your team, there are too many risks:
Once published as cleartext, your credit card number can easily leak, be copied to an insecure location, or be transmitted to unexpected third parties.
Even if you remove the critical piece of information from the repository later, it is still available from the project's history; that's the whole point of using Git.
Rewriting history, you said? Well, why not, but who would guarantee there aren't clones of the repository where the removed file is still available?
Not to mention the whole purpose of a code management system like Git is to remember the content of your files. So, even if you remove it later, once published, any piece of information remains accessible in the history.


What you wouldn't want to do with your credit card number, you don't want to do with your password, credentials, or API keys.
Less sensitive to run, your application may require configuration settings that vary from team member to team member or from one deployment environment to another.
Just to give you a few examples, those may be the local port to listen to for incoming requests or the detail log level provided in error messages.


I will encompass both your personal secrets and the host-specific configuration setting under the broad term of local settings here.
And we will see how we can decouple those local settings from the underlying application, both to ease deployment in various contexts and to protect critical information from leaking through your application.


In the first part of this article, we will focus on strategies to configure local settings on backend applications, where the deployment host is supposed to be under the full control of the developers and out of reach from the (potentially malicious) end-user.
In the second party, we will consider strategies that may provide the same facilities, but this time for the frontend part or your application. In this case, things are reversed since the execution environment is under the full control of a (potentially malicious) end-user


== In the backend

When working for the backend, you are always in control of the runtime environment. It's true all along the chain, from your development machine to the production deployment hosts.
On a correctly configured and secured server, an unauthorized user cannot directly access your application files nor examine the execution environment or your application.

So you can change a file or set up some environment variables to create target-specific configuration settings or secrets, and they won't be accessible from the outside world.

=== External config file

The simplest solution to manage your local settings is to store them in a configuration file.
The exact file format is of little importance as long as you can read it from your application:
for example, it could be a JSON file, a YAML file, or even a source file for your development language, as in the following example:

----
include::code/config.js[]
----

----
include::code/backend0.js[]
----

The critical point is you don't want to share your configuration file with the other developers and even less publish it out in the wild.
So, that file must NOT be tracked by your source code management tool.

I would encourage you to take all the required precautions to ensure this file would never be version controlled.
With git, this implies adding it to `.gitignore` and banning commands like `git add --all`.
Another option to prevent the accidental publication of your configuration file is to store it outside your application tree.
If you deploy on a Linux or BSD system and have root access to the deployment hosts, storing your local settings in `/etc` is a common practice.

This solution has the advantage of simplicity.

However, even if I left aside the risk of committing the configuration file by error, it also has some drawbacks.
First, I find this solution somewhat cumbersome when I need to provide default values.
Then, it quickly becomes problematic when you have to manage the configuration settings for different scenarios on the same host.
One might argue this problem could be solved by having one different container with specific settings for each scenario.

Indeed.
However, I still find these issues are elegantly solved using environment variables, as we will see now.


=== Enironment variables

I read here and there authors qualifying this as a legacy solution.
But it remains my favorite one because it works equally well in a variety of hosting and deployment situations.

An environment variable is a runtime entity that exists (supposedly in RAM) in the context of a running process.
Barring the underlying operating system limitations, you can store whatever you want in an environment variable:
for example, the port your server should listen to, the address of the HTTP proxy, or your private key to access an API.

On managed hosting, the deployment platform usually provides tools, either a CLI or a GUI, to set up the environment variables available to your application at runtime.
For example, on Heroku, you will use `heroku config:set ...` to create environment variables that will be accessible by your deployed application (https://devcenter.heroku.com/articles/config-vars)
Most if not all programming languages expose the environment variables so that you may access them from your code.
For example, `Node.js` offers the `process.env` object for that purpose:

----
include::code/backend1.js[]
----

Look especially at how the `port` constant is initialized.
This syntax means "take the value of the environment variable `MY_PORT` *or* (if unset, empty or zero) set it to 3000".
That is a way to provide a default value (`3000`) that can be overridden by an environment variable (`MY_PORT`).


You will set up environment variables on your local development computer using the built-in shell features.
For example, from a Bourne Shell (`bash`, `dash`, `sh`, ...), you may write:

----
export MY_PORT=3300
export MY_API_KEY='123$abc'

node backend1.js
----

You should see the server listen on the given port.
And if you point your browser to `http://localhost:3300`, you could see the server "using" your API key:

----
Example app listening on port 3300
Don't tell anyone, but our API key is 123$abc
----

The environment variables are inherited from their parent upon process creation.
That means once you define some environment variables in a shell, the child processes started by that shell will see them, and the grand-children started from child processes. And so on.


However, if you close the initial shell, your setup will be lost, and you will have to retype the two lines starting with the keyword `export` of the example above.

When you have many environment variables to set up that way, it is tempting to write them all in a file so that you can load all of them at once in your shell:

----
# All my environment variables are defined in that file
# (Bash syntax)
sh$ cat my.env
export MY_PORT=3300
export MY_API_KEY='123$abc'

# Load that file in the context of the current shell
sh$ source my.env

# Run the node application
sh$ node backend1.js
Example app listening on port 3300
----

Again, if you want to preserve your secret keys or credentials, you should take all the necessary measures to avoid adding that file to the version control system.
At the bare minimum, by blocklisting it:

----
echo my.env >> .gitignore
----

Or better, by storing that file outside of your working tree. Maybe in a directory only readable by you, the developer.
And if your deployment target does not provide facilities to configure environment variables, you may need to write a small shell script to bootstrap your application -- or consider a change of hosting provider.


[HACKR'NOTE]
====
Of course, nothing is perfect.

On Linux, notably, if I have access to your backend application's host, I may access its environment variable.
The trick is using the `/proc` filesystem that exposes many properties of running processes.

Here is an example. Let's first start the backend application and get its process ID (PID):

----
sh$ MY_API_KEY='123$abc' MY_PORT=3300 node backend1 &
sh$ NODE_PID=$!
sh$ echo $NODE_PID
588
----

The Linux kernel exposes the environment variable for a running process as a null separated list of strings in `/proc/<pid>/environ`.
Just reread that list, replacing the null character (`\0`) with a newline and filtering using `grep` to get back the variable of interest:

----
sh$ tr '\0' '\n' < /proc/$NODE_PID/environ | grep API_KEY
MY_API_KEY=123$abc
----

And I don't even have to be root for that! `/proc/<pid>/environ` is readable by the owner of the process:

----
sh$ ls -lsd /proc/588/environ 
0 -r-------- 1 sylvain sylvain 0 juin  13 16:03 /proc/588/environ
----

If you host several applications on the same machine, you should ensure each one is running under a different identity.
That way, an attacker exploiting a vulnerability in one of the applications wouldn't immediately gain access to the setting of all the other applications.

====


==== Permanent changes

Suppose you want to make your changes permanent. In that case, if you manage the deployment host by yourself, you will set up the environment variables through a combination of initialization scripts (maybe in `/etc/profile.d` or `~/.bashrc` / `~/.profile`) or global environment files (`/etc/environment`).
I will let you check the documentation for your specific operating system to learn more about the available options.

In all cases, you should ensure unauthorized users cannot access the content of these files if they contain credentials or cleartext passwords!

==== From a container

If you run your application in a container, your container runtime certainly provides a way to set up environment variables so they will be available to the containerized application. For example, you may use an `ENV` statement in a Dockerfile (which is probably not the best for sensitive information) or pass them with the `-e` flag of `docker run`.

----
sudo docker run -it --rm \
    -v "$PWD":/usr/src/app \
    -w /usr/src/app \
    -p 4000:4000 \
    -e MY_PORT=4000 \
    -e MY_API_KEY='123$abc' \
    node:16.15-alpine node backend1.js
----

Please note that storing sensitive pieces of information unencrypted in a container image is probably not the safest move.
In Docker, for example, it would just be a matter of using `docker inspect` to retrieve your API key.
Your container runtime may provide tools to deal with secrets securely (see https://earthly.dev/blog/docker-secrets/ for Docker). And we now have a whole range of applications specifically designed to deal with secret management.

=== Decouple secret management

Speaking of that, let me introduce the concept of decoupled secret management.
In this model, you interact with a third-party application to access your secrets, so they are no longer stored alongside your application sources or configuration settings.

The secret management tool will be responsible for securely storing and restricting access to the secrets.
It also avoids storing and transmitting cleartext secrets between applications or hosts.
Most evolved systems may also log secrets' usages, revoke credentials when they are no longer in use, or expire them after a certain time.


HashiCorp Vault is one of these solutions.
Despite many advantages, it is not exactly suited for the use case that interests me today (https://discuss.hashicorp.com/t/using-vault-to-store-personal-secret/40642).
So, I will avoid setting up a Vault instance just for that purpose here.
But now that you know the existence of such software, you may investigate more about it if needed.

== In the frontend

We will now leave the comfort of the backend where (almost) everything is under our control to the perilous lands of the frontend development, where nothing seems reliable.

In a frontend application, there are generally few configuration settings.
Except for an important class: the credentials, passwords, and other API keys your frontend may require to interact with other systems.
So, for the rest of the discussion, I will specifically focus on secrets (as pieces of information you want to protect) rather than talking about configuration settings at large.

First, and to make it clear once and for all, there is no way of protecting a secret in a frontend application.
Of course, you may use obfuscation so your Google Cloud API key will not be immediately visible.
But, if the client uses a secret, that secret must be readable client-side, and thus there is a way to capture that secret.

So, are we cursed? Should we resign to seeing our credit card charged for API calls made using a usurped key?
No, but the solution to choose will inevitably be a compromise between the complexity, the performance penalties, and the risks you accept to take.

=== Limit the scope of your credentials

The first step to prevent unauthorized use of your credentials, passwords, or API keys is to use every means the provider offers to limit their usage scope.
For example, if your application is designed to run only on a corporate intranet, you should check if your service or API provider offers a way to restrict the use of their credential to a given range of IP addresses.

Some providers, like Amazon AWS or Google Cloud, may offer hundreds of APIs and services.
If it's feasible, it's probably wiser to limit the scope of your keys or credentials only to the set of services your really need.

Finally, don't forget to set up alerts and budget limits to warn you if something goes wrong before it's too late.

=== buildtime injection

I said it if a user has full access to the host running your application, then that user may have access to the secrets used by your application.
So it's hopeless to search for a strategy to protect the secrets used in publicly deployed frontend applications.

But it's different during development.
You won't probably use the deployment credentials during development.
And you may even be in a situation where every developer will have their own set of personal credentials.
For example, imagine you work on an open-source project: you probably don't want to share your Google Maps API key with the rest of the world.

In that case, we may find solutions to protect your keys and credentials.

If your frontend application has a build step, one possible solution is to inject the credentials and other application settings  at build time:
You build the front end on your machine.
The credentials are injected into the application from environment variables or configuration files just like we did for the backend.
And you run/test the application locally on your machine with your credentials and settings.

At not any moment, you have to share your credential with someone else.
Of course, if you give access to the build application to a third person, they may capture your credentials.
But your credentials are safe as long as your application only runs on hosts whose access is limited to you.

In the example below, I added the `config` task to a `grub` buildfile to build my configuration file from the environment variables.

----
include::code/Grubfile
----

And you only have to ensure the proper environment variables are set when you build the application:

----
sh$ source my.env
sh$ ./gulp -L
sh$ cat build/config.js 

var api_key='123$abc';

----

I chose the simple approach of creating a `config.js` whose only purpose is to store the configuration generated at build time.
That file is loaded separately from the webpage:

----
include::code/src/index.html
----

Like for the backend, you may have different sets of settings and credentials for your different deployments: `development.env`, `test.env`, or even `production.env`.
Obviously, you should avoid sharing those files if you want to preserve your secrets and credentials.
That notably means you shouldn't track those files with your version control system (Git, Subversion, and so on).

You should also be aware that, if you decide to publicly deploy an application using this strategy to manage its configuration settings and secrets, you are de-facto making them public.

Instead of using a clearly visible configuration file, you might be tempted to rewrite some of your scripts at build time to embed your secrets.
Or you may add some extra steps to minify or somehow obfuscate your source files, making it less easy for a casual user to capture sensitive pieces of information.
But none of these strategies will protect you from someone determined to steal the secrets from your frontend application.
At the very worst, simply spying on the requests using some "developper tools" for your browser should reveal even the most carefully hidden API key!

Despite all these warnings, if you follow that way, you should closely monitor your web server logs.
If you see a sudden decorrelation between your credentials' usage and the number of times the configuration file was served, you should consider that as an indicator of potential abuse. And probably a strong signal that it's time to rotate your keys.


=== remote fetching

A variation of the above technique will consist in replacing the static injection of credentials and application settings at build-time with a dynamic resolution at runtime.
That sounds pretty impressive, isn't it?
But in practice, the things are simple: instead of returning a pre-generated file as we did above, this time, the configuration file is generated on the fly.


The major drawback of this solution is you now need to deploy a backend application, whereas, previously, a static web server was sufficient.
As a corollary, that makes things like caching or cross-region replication more complicated.

But it also has a few advantages:

* You may audit and automatically rotate or disable the credentials based on a specific threshold usage.
* You may update the credentials without having to touch in any way to the frontend application.
* You may restrict access to sensitive information only to users otherwise authenticated by your system.

Once again, let me repeat: it's just a matter of mitigating the risks.
And certainly not of removing it completely.

Here is an example of a basic Express application that serves static pages, except for the `config.js` file, which is server-side generated:

----
include code::backend2.js[]
----

The backend obtains the local settings from environment variables, as explained in the first part of this article.
Then it uses those pieces of data to dynamically generate the client configuration file:

----
sh$ MY_API_KEY='123$abc' MY_PORT=3400 node backend2.js
Example app listening on port 3400
----

Client-side, nothing distinguishes the configuration file from the static files server by the same backend:

----
sh$ curl http://localhost:3400/index.html
<html>
<head>
  <script src='config.js'></script>
  <script src='frontend.js'></script>
</head>
<body>
  <p><script>run()</script></p>
</body>
</html>
----

But when the browser queries the file `config.js`, it receives a server-side generated file:

----
sh$ curl http://localhost:3400/config.js

var api_key='123$abc';
----

=== Proxying

Finally, the only way to completely protect your secrets and credential is to not use them in the frontend application.
The solution here is to relay the requests coming from the frontend via your backend.
At the very least, the backend will behave like a proxy, simply injecting the API key on the fly when receiving a request.

Most interestingly, you may also take profit of that occasion to rethink your frontend-backend exchanges to reduce the traffic or abstract the frontend away from the data provider, so you might transparently change provider or aggregate data from several sources.
You may also implement caching on the backend to reduce the number of (paying) requests send to the underlying provider.
Finally, if you implement your own user authentication scheme, you can associate the requests to a given user and use that piece of information for API requests' accounting or rate-limiting.

But all this has a cost. By forcing all the traffic through your backend application, you introduce a single point of failure and a serious bottleneck.

In the example below, I set up an Express client to act as a proxy for the Google Cloud Places API.
To somewhat reduce the traffic and abstract the frontend from the underlying data provider, I've rewritten the response to provide only the necessary bits of information.
Error handling and response caching are left as an exercise to the reader:

embed::code/backend3.js[]

With this strategy, the frontend application no longer has to use the API key.
All it knows is the backend provides an endpoint to obtain the necessary pieces of data.
From the frontend perspective, it's indistinguishable if we provide the data ourselves or if we rely on a third-party provider.
In this example, there is still room for abusing our API endpoint and thus generating unexpected costs.
But at least our API key does not leak and thus can not be used for other purposes.

embed::code/src/index2.js[]

== Conclusion

TBD

== Resources

* https://withblue.ink/2021/05/07/storing-secrets-and-passwords-in-git-is-bad.html
* https://medium.com/@sheshbabu/ways-to-manage-config-in-frontend-and-their-tradeoffs-d7d3132803ea
